{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sea ice decline and Aitkensize particle relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuuli Lehmusjärvi (tuuli.lehmusjarvi@helsinki.fi)\n",
    "2 Nov, 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeGI course 2019 - Climate in high latitudes: eScience for linking Arctic measurements and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group assistant: Lisa Beck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aerosols have a direct and indirect effect on the radiative balance: direct effects are scattering, reflection and absorbtion of short wave radiation. THe indirect effect is occuring  when aerosol particles act as Cloud Condensation Nuclei (CCN) and affect the clound's properties such as life time and reflectivity. Currently atmospheric aerosols cause the larges uncertainties in global radiative forcing predictions and this is the biggest in the Arctic regions (Dall'Osto 2018).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New particle formation plays a big role in the CCN formation. In the Arctic summer new particle formation is the biggest and even though it occurs regionaly it affects worldwide aerosol number concentrations. New particles in the Arctic are mainly formed due to emission of biogenic sulphur gases(Dall'Osto 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course my task was to look atobservation data of particle size distribution from Zeppelin station at Ny-Ålesund and compare that to reanalyzed sea ice extend data. The aim was to see if sea ice loss is affecting the particle size distribution. Dall’Osto et al. 2018 claim that declining sea ice and therefore increased exposure of open water is increasing new particle formation in the Arctic. They were concentrating on observational data collected at Villum, North East Greenland. The idea is to study this effect in Svalbard. The reanalyzed data showed the ice extent in the whole Arctic sea from 1979-2012, but I wanted to concentrate on the area in the west of Svalbard (Fram Strait), which affects most to the measurements done at Zeppelin since it is located at the west coast. From the particle size distribution data from 2000-2016 I wanted to select the time periods when the wind was blowing from the west or north-west. With this filter I hoped to better see the link between the sea ice extent in the west of Svalbard and the particle concentration. However this turned out to be not as succesful as expected way and therefore to using all the observed and modelled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observational dataset of particle size distribution was from NILU and I got two datasets from EBAS (years 2000-2007 and 2008-2009) and the third dataset (2010-2016) from the NeGI server.(?)\n",
    "The reanalyzed dataset I used was ERA-Interim from ECMWF for the years 1979-2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook in methods section ([3](#Methods)) I will show how I analysed the both modelled and observed data and how I will comperare those two. In the results and discussion section ([4](#Results-and-discussion)) I will show my main finding and analysis from the data and parameters shown in methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading, analyzing and plottinf of the observational data is presented in  [Section 3.1](#Observational-data). [Section 3.2](#Reanalyzed-model-data) will show the same thing but with model data. All the code is commented to make it easier to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import all needed packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pyaerocom as pya\n",
    "import glob\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "from IPython.display import Image\n",
    "from tuuli_functions import size_dist_to_xarray\n",
    "from tuuli_functions import lognorm_to_concentration\n",
    "from tuuli_functions import load_seaice_xarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observational data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observational data used was aerosol particle size distribution data from Zeppelin station in Ny-Ålesund Svalbard. It was fetched from EBAS database. I wanted to have as long as possible time series and I ended up taking data from 2000 until 2015. A single long time series was not available in EBAS, but two smaller ones: first from 2000 to 2007 and second from 2008 to 2009. The third dataset I found was from our NEGi course folder and it was from 2010 to 2016. So now I had three different datasets which needed to be read separatly due to different file formats. I also ended up using the last dataset only until end of 2012, because the model data I used also ended in that year. In addition, there were many major data gaps in 2014 and 2015 which could have made the analysis unsure.  The particle size range in the data was from 20-500nm. Since I wanted to look the Aitkensize particles, I selected size range of 20-50nm from all the three datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also planned to use wind direction data from Zeppelin station from the same years I had the particle size distributions datasets (2000-2012). Eventually, I decided to drop the wind data analysis since it didn't work  how I was planned. In the subsection ([3.1.4](#Wind-data-from-Zeppelin)) the reading and processing of the wind data will still be shown, and in results and discussion ([4](#Results-and-discussion)) I will elaborate on the reasons of disregarding the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and cleaning the different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the datasets needed to be loaded into memory from the different files. After that all the flag values must be removed and new arrays created from the raw data. First the years 2000-2007:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading all the files in the firs part of the datasets (2000-2007)\n",
    "# together into a new array with pyaerocom\n",
    "arrs1 = []\n",
    "for filepath in glob.glob('EBAS_FILES_2000_2007/*.nas'):\n",
    "    filedata = pya.io.EbasNasaAmesFile(filepath)\n",
    "    arrs1.append(size_dist_to_xarray(filedata))    \n",
    "\n",
    "# Sorting the previously acquired array by time\n",
    "arr1 = arrs1[0]\n",
    "for array1 in arrs1[1:]:\n",
    "    arr1 = xr.concat([arr1, array1], dim='time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the years 2008-2009:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading all the files in the second part of the datasets (2008-2009)\n",
    "# together into a new array with pyaerocom\n",
    "arrs2 = []\n",
    "for filepath2 in glob.glob('ebasfiles_2008_2009/*.nas'):\n",
    "    filedata2 = pya.io.EbasNasaAmesFile(filepath2)\n",
    "    arrs2.append(size_dist_to_xarray(filedata2, step=1))\n",
    "    \n",
    "# Sorting the previously acquired array by time\n",
    "arr2 = arrs2[0]\n",
    "for array2 in arrs2[1:]:\n",
    "    arr2 = xr.concat([arr2, array2], dim='time')  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last the years 2010-2015. Here the reading is done little bit differently since the format of the files was .csv and easier to read with Pandas builtins than the .nas files of the two previous datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Reading the last part of datasets (2010-2015)\n",
    "datadir = '/home/2daa7756-2d5725-2d4dfb-2db0ff-2d5e0a6858a009/shared-ns1000k/inputs//Aerosol_sizedist_obs/'\n",
    "filenam1 = datadir + 'Zeppelin_2010_hourly.csv'\n",
    "filenam2 = datadir + 'Zeppelin_2011_hourly.csv'\n",
    "filenam3 = datadir + 'Zeppelin_2012_hourly.csv'\n",
    "filenam4 = datadir + 'Zeppelin_2013_hourly.csv'\n",
    "filenam5 = datadir + 'Zeppelin_2015_hourly.csv'\n",
    "\n",
    "flist=[filenam1, filenam2, filenam3, filenam4, filenam5]\n",
    "\n",
    "# Creating a date parser\n",
    "mydateparser = lambda x: pd.datetime.strptime(x, \"%Y %m %d %H %M\")\n",
    "\n",
    "# Go through each file and append them together. \n",
    "# Date is split in five first columns, so we parse them together using 'mydateparser'.\n",
    "\n",
    "datalist = []\n",
    "for f in flist:\n",
    "    datalist.append(pd.read_csv(f, parse_dates=[['0','0.1','0.2','0.3','0.4']],date_parser = mydateparser))\n",
    "# Convert the new 'datalist' to Paandas dataframe    \n",
    "data3 = pd.concat(datalist, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the first two datasets from raw data to readable arrays I used a function called 'size_dist_to_xarray': \n",
    "\n",
    "```python\n",
    "def size_dist_to_xarray(loaded_ebas_file, step=2 ):\n",
    "    filedata = loaded_ebas_file\n",
    "    all_d = filedata.data[:,2:-1:step]\n",
    "    time =  list(filedata.time_stamps)\n",
    "    ds = xr.Dataset({'time':time})\n",
    "\n",
    "    ds['sized'] = xr.DataArray(\n",
    "        all_d, \n",
    "        dims={'time':ds['time'], 'd_index': np.arange(len(all_d[0,:]))}\n",
    "    )\n",
    "    return ds['sized']\n",
    "```\n",
    "This function will remove the flag values from the raw data and returns it in an xarray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data which is stored in xarrays should be converted to Pandas dataframes for easier joining. I wanted them all be in the same format. Some of the data in the 2008-2009 dataset was clearly faulty. In this section, I also show the removal of these erroneous data points. The data in the other data sets was usable as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particle size distribution data doesn’t directly give the concentration of the particles. The instruments are measuring lognormat distributions so the unit of the 'concentrations' is actually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dN_i}{d\\log{D_p}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $N_i$ is the concentration of the particles, and $D_p$ is the diameter of the particles. So when processing the data I need to calculate the real concentration from the measured data. For that I used a funtion called 'lognorm_to_concentration'.The function parameters are the name of the dataframe with the cleaned data, start and end date from the dataset and the names of the starting column and ending column. These columns define the size range of the particles that I want to study (20-50). Now we can integrate over $log10(D_p)$ to get the real concentration value; the return value of the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get data for diameters within 20-50nm\n",
    "def lognorm_to_concentration(data, start_col, end_col, start_date, end_date):\n",
    "    df_sizes = data.loc[start_col:end_col,start_date:end_date].T\n",
    "\n",
    "# Integrate over log10(Dp) to get Ntot\n",
    "    Ntot = pd.Series(np.trapz(df_sizes,\n",
    "    x = np.log10(df_sizes.columns)),\n",
    "    index=df_sizes.index.copy())\n",
    "    \n",
    "    return Ntot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the first dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn arr1 to Pandas-dataframe for easier handling\n",
    "df1 = arr1.to_dataframe().unstack('d_index')\n",
    "cols = [x[1] for x in df1.columns.values]\n",
    "# Transpose the dataframe for the futher analysis\n",
    "df1_turned = df1.T.loc['sized']\n",
    "\n",
    "# Get data for 1 Mar 2000 - 31 Dec 2007 for diameters within 20-50nm\n",
    "# Column names 1-5 represents diameters from 20-50nm \n",
    "Ntot_20_50_2000_2007 = lognorm_to_concentration(df1_turned,1,5,'2000-03-01 00:30:00','2007-12-31 19:29:59')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the second dataset and removing incorrect data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn arr1 to Pandas-dataframe for easier handling\n",
    "df2 = arr2.to_dataframe().unstack('d_index')\n",
    "cols2 = [y[1] for y in df2.columns.values]\n",
    "\n",
    "# Masking the data to remove the overly large values which are not real\n",
    "\n",
    "dd=df2.iloc[:,3:8] #First selecting right particle size range (20-50nm) to a new dataframe dd\n",
    "mask = dd >10000 #Creating a mask of values over than 10000\n",
    "dd[mask] = np.nan #Setting those values to NaN\n",
    "\n",
    "# Plotting shows that in the end of 2009 there is weird peak\n",
    "dd.plot(figsize=(14, 6))\n",
    "plt.ylabel('Particle size distribution')\n",
    "plt.savefig('particle_size_08_09_not_converted.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](particle_size_08_09_not_converted.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting only the weird subset to see better\n",
    "weird_subset = dd[\"2008-12-01\":\"2008-12-24\"]\n",
    "weird_subset.plot(figsize=(14, 6))\n",
    "plt.ylabel('Particle size distribution')\n",
    "plt.savefig('particle_size_weird_subset.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](particle_size_weird_subset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting the time when the weird subset happened to NaN \n",
    "dd[\"2008-12-01\":\"2008-12-24\"] = np.nan\n",
    "\n",
    "# Transpose the dataframe for the futher analysis\n",
    "dd_turned = dd.T.loc['sized']\n",
    "\n",
    "# Get data for 1 Jan 2008 - 31 Dec 2009 for diameters within 20-50nm\n",
    "# Column names 3-7 represents diameters from 20-50nm \n",
    "Ntot_20_50_2008_2009 = lognorm_to_concentration(dd_turned,3,7,'2008-01-01 00:30:00','2009-12-31 23:29:59')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the third dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# When parsing the header is also affected so we give it a new name, 'date'\n",
    "data3.rename(columns={'0_0.1_0.2_0.3_0.4':'date'}, inplace = True)\n",
    "# Set indices of the rows to date\n",
    "data3 = data3.set_index('date')\n",
    "# Remove last column\n",
    "data3.drop(labels='0.6', axis=1, inplace=True) \n",
    "\n",
    "# Change all the incorrect values (-999) to NaN\n",
    "data3 = data3.replace(-999,np.nan)\n",
    "\n",
    "# Check the names of the columns, now they are the same as the diameter of the particles \n",
    "data3.columns = [float(ii) for ii in data3.columns]\n",
    "\n",
    "# Transpose the dataframe for the futher analysis\n",
    "new_d=data3.T\n",
    "# Get data for 1 Jan 2010 - 28 Aug 2013 for diameters within 20-50nm\n",
    "# Column names 20.0-50.238represents diameters from 20-50nm \n",
    "Ntot_20_50_2010_2013 = lognorm_to_concentration(new_d,20.0,50.238,'2010-01-01 00:00:00','2012-12-31 08:00:00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When processing data, it’s a good practice to have a look a the data as a sanity check to make sure everything is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data as a sanity check\n",
    "Ntot_20_50_2000_2007.plot(figsize=(12,6))\n",
    "plt.ylabel('Particle concentration')\n",
    "plt.savefig('sanitycheck_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sanitycheck_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data as a sanity check\n",
    "Ntot_20_50_2008_2009.plot(figsize=(12,6))\n",
    "plt.ylabel('Particle concentration')\n",
    "plt.savefig('sanitycheck_2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sanitycheck_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data as a sanity check\n",
    "Ntot_20_50_2010_2013.plot(figsize=(12,6))\n",
    "plt.ylabel('Particle concentration')\n",
    "plt.savefig('sanitycheck_3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sanitycheck_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind data from Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading ad cleaning the wind data is done much in the same way as the particle data before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all the wind direction files together into a new array with pyaerocom\n",
    "winddir = []\n",
    "for filepath4 in glob.glob('ebas_winddir/*.nas'):\n",
    "    filedata4 = pya.io.EbasNasaAmesFile(filepath4)\n",
    "    winddir.append(filedata4.to_dataframe().wind_direction_deg)\n",
    "    \n",
    "# Sorting the previously acquired array by time\n",
    "all_wind = winddir[0]\n",
    "for data in winddir[1:]:\n",
    "    all_wind = pd.concat([all_wind, data], axis=0)\n",
    "all_wind.sort_index(inplace=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the wind data as a mask to choose the correct days from the processed particle data, they need to have exactly the  same time frame. Here I created the filter mask to match the time of the particle data, and selected only the times when the wind came from between west and northwest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking hourly mean from the data.\n",
    "# Wind data and concentration data needs to start at the same time step that they can be used together\n",
    "all_wind_avg = all_wind.resample('H').mean()\n",
    "\n",
    "# Creating a mask for the wind directions. \n",
    "# Selecting only those times (hours) when wind comes from west or northwest\n",
    "filter_mask = all_wind_avg.between(260, 350) \n",
    "# Concentration starts on 1st of March 2000, so we need to make the wind data to start at the same time \n",
    "March_index = filter_mask.index > pd.to_datetime('2000-03-01 00:00:00')  \n",
    "new_filter = filter_mask[March_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reanalyzed model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the model data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
